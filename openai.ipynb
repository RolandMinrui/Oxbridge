{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sentiment analysis for OpenAI API and Azure API\n",
    "\n",
    "from utils import TestDataset, save_to_csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from openai import OpenAI,AzureOpenAI\n",
    "\n",
    "def inference(prompt, client, model):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "            max_tokens=4,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"!!!!! Error when processing prompt !!!!!\")\n",
    "        print(prompt)\n",
    "        # print(f\"!!!!! {e} !!!!!\")\n",
    "        return None\n",
    "\n",
    "def batch_inference(batch_data, client, model, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = list(\n",
    "            tqdm(\n",
    "                executor.map(lambda x: inference(x, client, model), batch_data),\n",
    "                total=len(batch_data)\n",
    "            )\n",
    "        )\n",
    "        for future in futures:\n",
    "            results.append(future)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Load Dataset -----\n",
      "----- Inference Deepseek -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:24<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Inference Azure GPT-4O-Mini -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 24/235 [00:03<00:24,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! Error when processing prompt !!!!!\n",
      "Instruction: Analyze the sentiment of this statement extracted from a financial news article. \n",
      "Provide your answer as either negative, positive or neutral without anything else.\n",
      "For instance, The company's stocks plummeted following the scandal. would be classified as negative.\n",
      "Text: $PKT Momentum building for breakout trigger ! Long setup http://stks.co/eXVU\n",
      "Response: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:14<00:00, 15.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# use appropriate dataset for inference\n",
    "# data_path = \"data/train_easy.csv\"\n",
    "# data_path = \"data/train_hard.csv\"\n",
    "# data_path = \"data/FPB.csv\"\n",
    "data_path = \"data/sentimental_analysis/FIQASA/test.csv\"\n",
    "\n",
    "prompt_template = '''Instruction: Analyze the sentiment of this statement extracted from a financial news article. \n",
    "Provide your answer as either negative, positive or neutral without anything else.\n",
    "For instance, The company's stocks plummeted following the scandal. would be classified as negative.\n",
    "Text: [Input Text]\n",
    "Response: '''\n",
    "\n",
    "print(\"----- Load Dataset -----\")\n",
    "test_dataset = TestDataset(data_path, prompt_template=prompt_template)\n",
    "batch_data = [test_dataset[i]['sentence'] for i in range(len(test_dataset))]\n",
    "\n",
    "print(\"----- Inference Deepseek -----\")\n",
    "deepseek = OpenAI(api_key=\"sk-619b8461a6e546d5a69c207e9b37ddc2\", base_url=\"https://api.deepseek.com\")\n",
    "deepseek_outputs = batch_inference(batch_data, deepseek, \"deepseek-chat\", 16)\n",
    "save_to_csv(data_path, \"deepseek\", deepseek_outputs)\n",
    "\n",
    "print(\"----- Inference Azure GPT-4O-Mini -----\")\n",
    "gpt = AzureOpenAI(api_key=\"b9135a15c242432cb20ddc43fea3a413\", api_version=\"2023-06-01-preview\", azure_endpoint=\"https://openai-oe.openai.azure.com/\")\n",
    "gpt_outputs = batch_inference(batch_data, gpt, \"gpt-4o-mini\", 16)\n",
    "save_to_csv(data_path, \"openai\", gpt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oxbridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
